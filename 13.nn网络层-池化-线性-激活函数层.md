

# 池化，线性，激活函数层



## 池化层 Pooling Layer

池化运算：对信号进行"收集"并“总结”，类似水池收集水资源，因而得名池化层。

“收集”：多变少 “总结”：最大值/平均值


![image](https://github.com/xiaoxingchen505/Pytorch-Notes/blob/master/images/pooling.png)


## nn.MaxPool2d

功能：对二维信号(图像) 进行最大值池化

<pre>
<code>
nn.MaxPool2d(kernel_size,stride= None,
             padding=0,dilation=1,
             return_indices=False,
             ceil_mode=False)
</code>
</pre>

主要参数：
* kernel_size: 池化核尺寸
* stride: 步长
* padding: 填充个数
* dilation: 池化核间隔大小
* ceil_mode: 尺寸向上取整
* return_indices: 记录池化像素索引

![image](https://github.com/xiaoxingchen505/Pytorch-Notes/blob/master/images/maxpooling.png)


## nn.AvgPool2d

功能：对二维信号进行平均值池化

<pre>
<code>
nn.AvgPool2d(kernel_size,
            stride= None,
             padding=0,
             ceil_mode=False
             count_include_pad=True,
             divisor_override=None,)
</code>
</pre>

主要参数:
* kernel_size: 池化核尺寸
* stride: 步长
* padding: 填充个数
* dilation: 池化核间隔大小
* ceil_mode: 尺寸向上取整
* count_include_pad: 填充值用于计算
* divisor_override: 除法因子


## nn.MaxUnpool2d

<pre>
<code>
nn.MaxPool2d(kernel_size,
            stride= None,
             padding=0)
</code>
</pre>


功能：对二维信号(图像) 进行最大值池化上采样

主要参数：
* kernel_size: 池化核尺寸
* stride: 步长
* padding: 填充个数

![image](https://github.com/xiaoxingchen505/Pytorch-Notes/blob/master/images/maxunpool.png)

在进行反池化之前，必须在池化层设置return_indices= True,并且需要额外赋值给一个indices，这样在反池化时，可以将indices作为参数传入。


## 线性层


线性层又称为全连接层，其每个神经元都与上一层所有神经元相连实现对前一层的线性组合，线性变换。

### nn.Linear

<pre>
<code>
nn.Linear(in_features, out_features, bias =True)
</code>
</pre>
功能：对一维信息(向量)进行线性组合
主要参数：
* in_features:输入结点数
* out_features:输出结点数
* bias: 是否需要偏置

公式：y= x*W_T + bias

## 激活函数层 activation layer

激活函数对特征进行非线性变换，赋予多层神经网络具有深度的意义。
![image](https://github.com/xiaoxingchen505/Pytorch-Notes/blob/master/images/activationlayer.png)

### nn.sigmoid
![image](https://github.com/xiaoxingchen505/Pytorch-Notes/blob/master/images/sigmoid.png)

### nn.tanh
![image](https://github.com/xiaoxingchen505/Pytorch-Notes/blob/master/images/tanh.png)

### nn.ReLU
![image](https://github.com/xiaoxingchen505/Pytorch-Notes/blob/master/images/relu.png)

### 几种改进的激活函数
![image](https://github.com/xiaoxingchen505/Pytorch-Notes/blob/master/images/rrelu.png)
