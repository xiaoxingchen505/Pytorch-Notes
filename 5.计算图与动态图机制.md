

# 计算图与动态图机制 Computational Graph

## 计算图是用来描述运算的有向无环图


计算图有两个主要元素：结点 Node，边 Edge

结点表示数据，如向量，矩阵，张量，边表示运算，如加减乘除卷积等。

用计算图表示: y = (x+w) *(w+1)

a = x+w, b = w+1

y = a * b

计算图与梯度求导:

dy/dw = dy/da * da/dw + dy/db * db/dw

      = b * 1 + a * 1
      = b + a
      = (w+1) + (x+w)
      = 2*w + x + 1
      = 2*1 + 2 + 1  = 5

![image](https://github.com/xiaoxingchen505/Pytorch-Notes/blob/master/%E8%AE%A1%E7%AE%97%E5%9B%BE.png)


## 对应代码
<pre>
    <code>
import torch

w = torch.tensor([1.], requires_grad = True)
x = torch.tensor([2.], requires_grad = True)

a = torch.add(w,x)   
b = torch.add(w,1)
y = torch.mul(a,b)

y.backward()
print(w.grad)
    </code>
</pre>

## 叶子结点：用户创建的结点称为叶子结点，比如X 与 W

### torch.Tensor(data,dtype,shape,device,requires_grad,grad,grad_fn,is_leaf)

is_leaf: 指示张量是否为叶子结点

叶子结点的作用是能够帮助节省内存，因为在梯度反向传播结束以后，非叶子结点的梯度，是会被释放掉的。

<pre>
    <code>

# 查看叶子结点
print("is_lead:\n", w.is_leaf, x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)

#查看梯度
print("gradient:\n", w.grad, x.grad, a.grad, b.grad, y.grad)
    </code>
</pre>

## retain_grad()
可以保存对应张量的梯度
<pre>
    <code>
a.retain_grad()
    </code>
</pre>

## grad_fn: 记录创建该张量锁使用的方法
<pre>
    <code>
y.grad_fn = <MulBackward0>
a.grad_fn = <AddBackward0>
b.grad_fn = <AddBackward0>
    </code>
</pre>

## 动态图

静态图： 先搭建图，后运算,  Tensorflow
特点：高效，不灵活

动态图：运算与搭建同时进行， Pytorch
特点：灵活，方便对每一层计算进行调整。