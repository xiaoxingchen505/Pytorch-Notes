

# 容器

## containers


![image](https://github.com/xiaoxingchen505/Pytorch-Notes/blob/master/images/nn2.png)


## nn.Sequential

nn.Sequential 是 nn.module的容器，用于按顺序包装一组网络层。

* 顺序性: 各网络层之间严格按照顺序构建。
* 自带forward(): 自带的forward里，通过for循环依次执行前向传播运算。

![image](https://github.com/xiaoxingchen505/Pytorch-Notes/blob/master/images/nn3.png)

<pre>
<code>
class LeNetSequential(nn.Module):
    def __init__(self, classes):
        super(LeNetSequential, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 6, 5),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(6, 16, 5),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),)

        self.classifier = nn.Sequential(
            nn.Linear(16*5*5, 120),
            nn.ReLU(),
            nn.Linear(120, 84),
            nn.ReLU(),
            nn.Linear(84, classes),)

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size()[0], -1)
        x = self.classifier(x)
        return x

# 网络层进行命名
class LeNetSequentialOrderDict(nn.Module):
    def __init__(self, classes):
        super(LeNetSequentialOrderDict, self).__init__()

        self.features = nn.Sequential(OrderedDict({
            'conv1': nn.Conv2d(3, 6, 5),
            'relu1': nn.ReLU(inplace=True),
            'pool1': nn.MaxPool2d(kernel_size=2, stride=2),

            'conv2': nn.Conv2d(6, 16, 5),
            'relu2': nn.ReLU(inplace=True),
            'pool2': nn.MaxPool2d(kernel_size=2, stride=2),
        }))

        self.classifier = nn.Sequential(OrderedDict({
            'fc1': nn.Linear(16*5*5, 120),
            'relu3': nn.ReLU(),

            'fc2': nn.Linear(120, 84),
            'relu4': nn.ReLU(inplace=True),

            'fc3': nn.Linear(84, classes),
        }))

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size()[0], -1)
        x = self.classifier(x)
        return x
    
# net = LeNetSequential(classes=2)
# net = LeNetSequentialOrderDict(classes=2)
#
# fake_img = torch.randn((4, 3, 32, 32), dtype=torch.float32)
#
# output = net(fake_img)
</code>
</pre>

## nn.ModuleList

nn.ModuleList是 nn.module的容器, 用于包装一组网络层，以迭代方式调用网络层

主要方法：
* append()：在ModuleList后面添加网络层
* extend(): 拼接两个moduleList
* insert(): 指定在ModuleList中位置插入网络层



<pre>
<code>

class ModuleList(nn.Module):
    def __init__(self):
        super(ModuleList, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(20)])

    def forward(self, x):
        for i, linear in enumerate(self.linears):
            x = linear(x)
        return x

# net = ModuleList()
#
# print(net)
#
# fake_data = torch.ones((10, 10))
#
# output = net(fake_data)
</code>
</pre>


## nn.ModuleDict

nn.ModuleDict 是 nn.module的容器，用于包装一组网络层，以索引方式调用网络层
主要方法：

* clear(): 清空ModuleDict
* items(): 返回可迭代的键值对 (key-value pairs)
* keys(): 返回字典的键(key)
* values(): 返回字典的值(value)
* pop(): 返回一对键值，并从字典中删除

<pre>
<code>
class ModuleDict(nn.Module):
    def __init__(self):
        super(ModuleDict, self).__init__()
        self.choices = nn.ModuleDict({
            'conv': nn.Conv2d(10, 10, 3),
            'pool': nn.MaxPool2d(3)
        })

        self.activations = nn.ModuleDict({
            'relu': nn.ReLU(),
            'prelu': nn.PReLU()
        })

    def forward(self, x, choice, act):
        x = self.choices[choice](x)
        x = self.activations[act](x)
        return x


net = ModuleDict()

fake_img = torch.randn((4, 10, 32, 32))

output = net(fake_img, 'conv', 'relu')

print(output)

</code>
</pre>


## 容器总结

* nn.Sequential: 顺序性， 各网络层之间严格按顺序执行，常用语block构建。

* nn.ModuleList: 迭代性， 常用于大量重复网构建，通过for循环实现重复构建。

* nn.ModuleDict: 索引性, 常用语可选择的网络层。


## AlexNet的构建

AlexNet的特点如下：

1. 采用Relu: 替换饱和激活函数，减轻梯度消失
2. 采用LRN (Local Response Normalization): 对数据归一化，减轻梯度消失.
3. Dropout: 提高全连接层的鲁棒性，增加网络的泛化能力
4. Data Augnment: TenCrop，色彩修改

![image](https://github.com/xiaoxingchen505/Pytorch-Notes/blob/master/images/alexnet.png)

<pre>
<code>
alexnet = torchvision.models.AlexNet()

</code>
</pre>